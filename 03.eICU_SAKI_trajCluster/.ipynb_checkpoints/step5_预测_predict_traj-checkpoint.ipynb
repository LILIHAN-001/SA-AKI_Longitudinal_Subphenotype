{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d1af0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T02:30:57.893691Z",
     "start_time": "2023-01-09T02:30:54.766888Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error,mean_absolute_error, f1_score\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.ensemble import ExtraTreesRegressor as etr\n",
    "from sklearn.linear_model import BayesianRidge as br\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.linear_model import ElasticNet as en\n",
    "from sklearn.kernel_ridge import KernelRidge as kr\n",
    "from sklearn.model_selection import  KFold, StratifiedKFold,GroupKFold, RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "import logging\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "warnings.filterwarnings('ignore') #消除warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b23d85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T02:31:02.149357Z",
     "start_time": "2023-01-09T02:31:02.146516Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8a4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T02:06:36.583692Z",
     "start_time": "2023-01-09T02:06:36.580161Z"
    }
   },
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold,cross_val_score,GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24cc292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T06:32:21.015052Z",
     "start_time": "2023-01-08T06:32:21.012203Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bca85f13",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d3bfa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T02:31:14.529974Z",
     "start_time": "2023-01-09T02:31:13.618625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7134, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_type = pd.read_csv(\"df_mixAK_fea3_C3.csv\")\n",
    "df_type_filt = df_type.loc[:,[\"stay_id\",\"groupHPD\"]]\n",
    "df_type_filt = df_type_filt.drop_duplicates()\n",
    "\n",
    "df_fea = pd.read_csv(\"sk_feature_timescale_addPastdata.csv\")\n",
    "df_fea = df_fea[df_fea[\"time\"].isin([-1])]\n",
    "\n",
    "df_fea = pd.merge(df_fea,df_type_filt, how=\"inner\",on=\"stay_id\")\n",
    "df_fea = df_fea.drop([\"time\"], axis=1)\n",
    "df_fea.shape\n",
    "\n",
    "df_fea.loc[df_fea[\"groupHPD\"]==3,\"groupHPD\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d753f255",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T06:49:47.165498Z",
     "start_time": "2023-01-09T06:49:47.151143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5707, 62), (1427, 62))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_features = ['heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate','temperature', 'spo2', 'glucose', 'gcs', 'gcs_motor', 'gcs_verbal','gcs_eyes', 'gcs_unable', 'so2', 'po2', 'pco2', 'fio2', 'aado2','pao2fio2ratio', 'ph', 'baseexcess', 'bicarbonate', 'chloride','hematocrit', 'hemoglobin', 'potassium', 'sodium', 'wbc','free_calcium', 'calcium', 'totalco2', 'lactate', 'basophils_abs','eosinophils_abs', 'lymphocytes_abs', 'monocytes_abs','neutrophils_abs', 'basophils', 'eosinophils', 'lymphocytes','monocytes', 'neutrophils', 'aniongap', 'bun', 'creatinine', 'inr','pt', 'ptt', 'mch', 'mchc', 'mcv', 'platelet', 'rbc', 'rdw', 'alt','alp', 'ast', 'bilirubin_total', 'sofa', 'urineoutput', 'renal_sofa','crea_divide_basecrea']\n",
    "\n",
    "X = df_fea \n",
    "X.index = pd.RangeIndex(len(X.index))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_, X_test_ = train_test_split(X, test_size=0.2, random_state=42) #random_state:设置随机种子，保证每次运行生成相同的随机数\n",
    "X_train_.index = pd.RangeIndex(len(X_train_.index))\n",
    "X_test_.index = pd.RangeIndex(len(X_test_.index))\n",
    "\n",
    "X_train = X_train_[use_features]\n",
    "y_train = X_train_.loc[:,[\"groupHPD\"]]\n",
    "\n",
    "X_test = X_test_[use_features]\n",
    "y_test = X_test_.loc[:,[\"groupHPD\"]]\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02196eb9",
   "metadata": {},
   "source": [
    "# lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_logloss：不仅考虑模型最后的分类表现，它还考虑了模型的概率。更大的概率会使得log loss变小，直至接近0；\n",
    "# auc_mu: auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "144d6252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T07:30:03.191530Z",
     "start_time": "2023-01-09T07:29:50.859572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's auc_mu: 0.963257\ttraining's multi_logloss: 0.328862\tvalid_1's auc_mu: 0.920668\tvalid_1's multi_logloss: 0.407207\n",
      "[1000]\ttraining's auc_mu: 0.985245\ttraining's multi_logloss: 0.251126\tvalid_1's auc_mu: 0.920275\tvalid_1's multi_logloss: 0.401305\n",
      "[1500]\ttraining's auc_mu: 0.994705\ttraining's multi_logloss: 0.196825\tvalid_1's auc_mu: 0.91873\tvalid_1's multi_logloss: 0.40425\n",
      "Early stopping, best iteration is:\n",
      "[798]\ttraining's auc_mu: 0.978579\ttraining's multi_logloss: 0.278255\tvalid_1's auc_mu: 0.921267\tvalid_1's multi_logloss: 0.401161\n",
      "\n",
      "fold n°2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=16 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.016 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=2 will be ignored. Current value: lambda_l2=0.2\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's auc_mu: 0.966234\ttraining's multi_logloss: 0.321035\tvalid_1's auc_mu: 0.907613\tvalid_1's multi_logloss: 0.419763\n",
      "[1000]\ttraining's auc_mu: 0.986454\ttraining's multi_logloss: 0.242618\tvalid_1's auc_mu: 0.905108\tvalid_1's multi_logloss: 0.417749\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's auc_mu: 0.974837\ttraining's multi_logloss: 0.290273\tvalid_1's auc_mu: 0.907982\tvalid_1's multi_logloss: 0.416153\n",
      "\n",
      "fold n°3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=16 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.016 will be ignored. Current value: lambda_l1=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.2, reg_lambda=2 will be ignored. Current value: lambda_l2=0.2\n",
      "Training until validation scores don't improve for 800 rounds\n",
      "[500]\ttraining's auc_mu: 0.962316\ttraining's multi_logloss: 0.322617\tvalid_1's auc_mu: 0.91085\tvalid_1's multi_logloss: 0.423759\n",
      "[1000]\ttraining's auc_mu: 0.984828\ttraining's multi_logloss: 0.246358\tvalid_1's auc_mu: 0.912873\tvalid_1's multi_logloss: 0.418018\n",
      "[1500]\ttraining's auc_mu: 0.994675\ttraining's multi_logloss: 0.192307\tvalid_1's auc_mu: 0.912331\tvalid_1's multi_logloss: 0.419382\n",
      "Early stopping, best iteration is:\n",
      "[1155]\ttraining's auc_mu: 0.988818\ttraining's multi_logloss: 0.227964\tvalid_1's auc_mu: 0.913643\tvalid_1's multi_logloss: 0.417189\n",
      "\n",
      "CV score: 0.37375153\n",
      "训练集的AUC score: 0.84650 \n",
      "测试集的AUC score: 0.84723 \n"
     ]
    }
   ],
   "source": [
    "#lightGBM决策树\n",
    "lgb_param = {\n",
    "    'objective': 'multiclass',\n",
    "    \"num_class\":3, # 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 7, \n",
    "    'min_data_in_leaf': 20, #叶子可能具有的最小记录数\n",
    "    \"boosting\": \"gbdt\", #用gbdt算法\n",
    "    \"feature_fraction\": 0.4, #例如 0.18时，意味着在每次迭代中随机选择18％的参数来建树\n",
    "    \"bagging_freq\": 9,\n",
    "    \"bagging_fraction\": 0.6, #每次迭代时用的数据比例\n",
    "    #\"bagging_seed\": 14,\n",
    "    \"min_child_samples\":16,\n",
    "    \"min_child_weight\":0.001,\n",
    "    \"metric\":{'multi_logloss','auc_mu'}, # multi_error:多分类错误率; https://blog.csdn.net/weixin_43440760/article/details/108843033\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"lambda_l2\": 0.2,\n",
    "    \"reg_alpha\":0.016,\n",
    "    \"reg_lambda\":2,\n",
    "    \"cat_smooth\":0,\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"lambda_l2\": 0.2, \n",
    "    \"verbosity\": -1}\n",
    "                            \n",
    "folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=4)   #交叉切分\n",
    "oof_lgb = np.zeros(len(X_train))\n",
    "predictions_lgb = np.zeros(len(X_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, y_train.values)):\n",
    "    print(\"\\nfold n°{}\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx]) # 转换为Dataset数据格式\n",
    "    val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx]) \n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(lgb_param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 800) \n",
    "    oof_lgb[val_idx] = [list(x).index(max(x)) for x in clf.predict(X_train.iloc[val_idx], num_iteration=clf.best_iteration)] # clf.predict(X_train.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    predictions_lgb = [list(x).index(max(x)) for x in clf.predict(X_test, num_iteration=clf.best_iteration)] # clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"\\nCV score: {:<8.8f}\".format(mean_squared_error(oof_lgb, y_train))) #K个模型分别在验证集中评估结果，最后的误差MSE(Mean Squared Error)加和平均就得到交叉验证误差。\n",
    "print(\"训练集的AUC score: {:<8.5f}\".format(metrics.accuracy_score(y_train, oof_lgb)))\n",
    "print(\"测试集的AUC score: {:<8.5f}\".format(metrics.accuracy_score(y_test, predictions_lgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb1ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题：1）多分类的评价标准；auc_mu即可\n",
    "# CV值的大小说明什么？交叉验证时结果取什么\n",
    "我们通过K-Fold 多次划分的形式进行训练是为了获取某个模型的性能指标，单一K-Fold训练的模型无法表示总体性能，但是我们可以通过K-Fold训练的训练记录下来较为优异的超参数，然后再以最优模型最优参数进行重新训练，将会取得更优结果。\n",
    "我们可以将5个概率矩阵直接求平均后做二分类预测，也可以分别做完二分类预测，再做投票，来获得最终的多类预测结果 #refer: https://zhuanlan.zhihu.com/p/67986077\n",
    "\n",
    "# 接下来是调参，然后是当比较各模型好坏时，单模型需要提供什么（F1, AUC, recall，precision）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41265e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb调参\n",
    "import lightgbm as lgb\n",
    "from lightgbm import Dataset\n",
    "parameters = {\n",
    "     'max_depth': [4,6,8],\n",
    "     'num_leaves': [20,30,40],\n",
    " }\n",
    "\n",
    "gbm = lgb.LGBMClassifier(objective = 'multiclass',\n",
    "                        num_class=3,\n",
    "                          is_unbalance = True,\n",
    "                          metric = 'auc',\n",
    "                          max_depth = 3,\n",
    "                          num_leaves = 4,\n",
    "                          learning_rate = 0.009,\n",
    "                          feature_fraction = 0.3,\n",
    "                          min_child_samples=16,\n",
    "                          min_child_weight=0.001,\n",
    "                          bagging_fraction = 0.6,\n",
    "                          bagging_freq = 9,\n",
    "                          reg_alpha = 0.016,\n",
    "                          reg_lambda = 2,\n",
    "                          cat_smooth = 0,\n",
    "                          num_iterations = 1000,   \n",
    "                         )\n",
    "gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='roc_auc', cv=5, verbose=0)\n",
    "gsearch.fit(X_train, y_train)\n",
    "print('参数的最佳取值:{0}'.format(gsearch.best_params_),'最佳模型得分:{0}'.format(gsearch.best_score_))\n",
    "print(gsearch.cv_results_['mean_test_score'],gsearch.cv_results_['params'])\n",
    "\n",
    "# #1    'max_depth': [4,6,8],\n",
    "#     # 'num_leaves': [20,30,40],\n",
    "# #2 'min_child_samples': [18,19,20,21,22],\n",
    "#     # 'min_child_weight': [0.001,0.002],\n",
    "# #3  'feature_fraction': [0.6, 0.8, 1],\n",
    "# #4   'bagging_fraction': [0.8,0.9,1],\n",
    "#      #'bagging_freq': [2,3,4],\n",
    "# #5  'reg_alpha':[],\n",
    "#     #'reg_lambda':[]\n",
    "# #6  'cat_smooth': [0,10,20],\n",
    "# #7  'learning_rate':\n",
    "# #8  'num_iterations':[1000,5000,8000,10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9995eb",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2499f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c811a3f",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd150087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T07:09:44.237495Z",
     "start_time": "2023-01-08T07:09:44.177841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([   0,    1,    3,    4,    5,    6,    7,    8,    9,   11,\\n            ...\\n            5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706],\\n           dtype='int64', length=4565)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_, (trn_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(folds\u001b[38;5;241m.\u001b[39msplit(X_train, y_train)):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold n°\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(fold_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 22\u001b[0m     trn_data \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrn_idx\u001b[49m\u001b[43m]\u001b[49m, y_train[trn_idx])\n\u001b[1;32m     23\u001b[0m     val_data \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_train[val_idx], y_train[val_idx])\n\u001b[1;32m     25\u001b[0m     watchlist \u001b[38;5;241m=\u001b[39m [(trn_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m), (val_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_data\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[0;32m/home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/pandas/core/frame.py:3030\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3029\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3030\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/pandas/core/indexing.py:1266\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1264\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_read_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/home/hanl/miniconda3/envs/mytensor/lib/python3.9/site-packages/pandas/core/indexing.py:1308\u001b[0m, in \u001b[0;36m_LocIndexer._validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m   1307\u001b[0m     axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1310\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m# We (temporarily) allow for some missing keys with .loc, except in\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;66;03m# some cases (e.g. setting) in which \"raise_missing\" will be False\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([   0,    1,    3,    4,    5,    6,    7,    8,    9,   11,\\n            ...\\n            5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706],\\n           dtype='int64', length=4565)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "##### xgb_263\n",
    "#xgboost\n",
    "xgb_263_params = {'eta': 0.02,  #lr\n",
    "              'max_depth': 6,  \n",
    "              'min_child_weight':3,#最小叶子节点样本权重和\n",
    "              'gamma':0, #指定节点分裂所需的最小损失函数下降值。\n",
    "              'subsample': 0.7,  #控制对于每棵树，随机采样的比例\n",
    "              'colsample_bytree': 0.3,  #用来控制每棵随机采样的列数的占比 (每一列是一个特征)。\n",
    "              'lambda':2,\n",
    "              'objective': 'reg:linear', \n",
    "              'eval_metric': 'rmse', \n",
    "              'silent': True, \n",
    "              'nthread': -1}\n",
    "\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_xgb_263 = np.zeros(len(X_train))\n",
    "predictions_xgb_263 = np.zeros(len(X_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(X_train[trn_idx], y_train[trn_idx])\n",
    "    val_data = xgb.DMatrix(X_train[val_idx], y_train[val_idx])\n",
    "\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "    xgb_263 = xgb.train(dtrain=trn_data, num_boost_round=3000, evals=watchlist, early_stopping_rounds=600, verbose_eval=500, params=xgb_263_params)\n",
    "    oof_xgb_263[val_idx] = xgb_263.predict(xgb.DMatrix(X_train_263[val_idx]), ntree_limit=xgb_263.best_ntree_limit)\n",
    "    predictions_xgb_263 += xgb_263.predict(xgb.DMatrix(X_test_263), ntree_limit=xgb_263.best_ntree_limit) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_xgb_263, target)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076144d7",
   "metadata": {},
   "source": [
    "# RandomForestRegressor随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7b0795c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-08T07:10:06.768979Z",
     "start_time": "2023-01-08T07:09:47.426301Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1600 out of 1600 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.13892259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1600 out of 1600 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#RandomForestRegressor随机森林\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof_rfr_263 = np.zeros(len(X_train))\n",
    "predictions_rfr_263 = np.zeros(len(X_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train.iloc[trn_idx]\n",
    "    tr_y = y_train.iloc[trn_idx]\n",
    "    rfr_263 = rfr(n_estimators=1600,max_depth=9, min_samples_leaf=9, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.25,verbose=1,n_jobs=-1) #并行化\n",
    "    #verbose = 0 为不在标准输出流输出日志信息\n",
    "    #verbose = 1 为输出进度条记录\n",
    "    #verbose = 2 为每个epoch输出一行记录\n",
    "    rfr_263.fit(tr_x,tr_y)\n",
    "    oof_rfr_263[val_idx] = rfr_263.predict(X_train.iloc[val_idx])\n",
    "    \n",
    "    predictions_rfr_263 += rfr_263.predict(X_test) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_rfr_263, target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af4b29",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor梯度提升决策树\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63c8fa6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:01:15.245948Z",
     "start_time": "2022-12-06T09:01:08.662703Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3152           0.0015            1.26s\n",
      "         2           0.3284           0.0011            1.25s\n",
      "         3           0.3263           0.0017            1.25s\n",
      "         4           0.3088           0.0006            1.26s\n",
      "         5           0.2754           0.0013            1.27s\n",
      "         6           0.3076           0.0012            1.27s\n",
      "         7           0.3325           0.0015            1.27s\n",
      "         8           0.2681           0.0016            1.26s\n",
      "         9           0.3039           0.0013            1.25s\n",
      "        10           0.3082           0.0013            1.25s\n",
      "        20           0.2869           0.0015            1.18s\n",
      "        30           0.2740           0.0005            1.16s\n",
      "        40           0.2493           0.0005            1.12s\n",
      "        50           0.2471           0.0006            1.09s\n",
      "        60           0.2284           0.0008            1.07s\n",
      "        70           0.2445           0.0001            1.04s\n",
      "        80           0.2161           0.0005            1.01s\n",
      "        90           0.2091           0.0001            0.98s\n",
      "       100           0.1980           0.0002            0.95s\n",
      "       200           0.1375          -0.0000            0.64s\n",
      "       300           0.1066          -0.0001            0.32s\n",
      "       400           0.0755          -0.0001            0.00s\n",
      "fold n°2\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3055           0.0019            1.19s\n",
      "         2           0.2927           0.0016            1.22s\n",
      "         3           0.2683           0.0021            1.24s\n",
      "         4           0.2636           0.0023            1.28s\n",
      "         5           0.3383           0.0012            1.27s\n",
      "         6           0.2945           0.0013            1.25s\n",
      "         7           0.3141           0.0018            1.25s\n",
      "         8           0.2759           0.0012            1.25s\n",
      "         9           0.2844           0.0013            1.24s\n",
      "        10           0.2978           0.0012            1.23s\n",
      "        20           0.2581           0.0015            1.19s\n",
      "        30           0.2656           0.0010            1.16s\n",
      "        40           0.2498           0.0009            1.13s\n",
      "        50           0.2103           0.0009            1.11s\n",
      "        60           0.2042           0.0003            1.07s\n",
      "        70           0.1953           0.0002            1.04s\n",
      "        80           0.1856           0.0006            1.02s\n",
      "        90           0.2024           0.0006            0.99s\n",
      "       100           0.1719           0.0001            0.96s\n",
      "       200           0.1271           0.0001            0.64s\n",
      "       300           0.0981          -0.0001            0.32s\n",
      "       400           0.0675          -0.0001            0.00s\n",
      "fold n°3\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3051           0.0011            1.15s\n",
      "         2           0.3222           0.0011            1.21s\n",
      "         3           0.3230           0.0017            1.25s\n",
      "         4           0.3082           0.0008            1.27s\n",
      "         5           0.3037           0.0014            1.28s\n",
      "         6           0.2994           0.0013            1.27s\n",
      "         7           0.2720           0.0015            1.26s\n",
      "         8           0.3387           0.0017            1.26s\n",
      "         9           0.3101           0.0014            1.24s\n",
      "        10           0.2979           0.0016            1.24s\n",
      "        20           0.2710           0.0008            1.19s\n",
      "        30           0.2653           0.0007            1.17s\n",
      "        40           0.2452           0.0008            1.13s\n",
      "        50           0.2419           0.0005            1.10s\n",
      "        60           0.2158           0.0005            1.07s\n",
      "        70           0.2041           0.0004            1.04s\n",
      "        80           0.2025           0.0004            1.01s\n",
      "        90           0.1848           0.0000            0.97s\n",
      "       100           0.1965           0.0005            0.94s\n",
      "       200           0.1326          -0.0001            0.63s\n",
      "       300           0.1048          -0.0001            0.32s\n",
      "       400           0.0796          -0.0000            0.00s\n",
      "fold n°4\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3178           0.0010            1.11s\n",
      "         2           0.3329           0.0012            1.21s\n",
      "         3           0.2950           0.0018            1.24s\n",
      "         4           0.3015           0.0021            1.26s\n",
      "         5           0.2687           0.0022            1.25s\n",
      "         6           0.2820           0.0024            1.25s\n",
      "         7           0.3078           0.0019            1.26s\n",
      "         8           0.2772           0.0017            1.26s\n",
      "         9           0.3094           0.0018            1.27s\n",
      "        10           0.3171           0.0017            1.26s\n",
      "        20           0.2517           0.0014            1.20s\n",
      "        30           0.2967           0.0009            1.17s\n",
      "        40           0.2517           0.0009            1.13s\n",
      "        50           0.2377           0.0006            1.10s\n",
      "        60           0.2099           0.0006            1.07s\n",
      "        70           0.2107           0.0005            1.04s\n",
      "        80           0.1883           0.0006            1.01s\n",
      "        90           0.1967           0.0004            0.98s\n",
      "       100           0.1736           0.0002            0.95s\n",
      "       200           0.1496           0.0000            0.64s\n",
      "       300           0.0893           0.0000            0.32s\n",
      "       400           0.0842          -0.0001            0.00s\n",
      "fold n°5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.3274           0.0018            1.14s\n",
      "         2           0.3270           0.0021            1.21s\n",
      "         3           0.2941           0.0020            1.23s\n",
      "         4           0.3305           0.0017            1.25s\n",
      "         5           0.3170           0.0017            1.23s\n",
      "         6           0.3157           0.0015            1.24s\n",
      "         7           0.3229           0.0010            1.25s\n",
      "         8           0.2631           0.0016            1.24s\n",
      "         9           0.3302           0.0013            1.23s\n",
      "        10           0.3030           0.0015            1.23s\n",
      "        20           0.2608           0.0009            1.18s\n",
      "        30           0.2527           0.0011            1.14s\n",
      "        40           0.2255           0.0004            1.11s\n",
      "        50           0.2416           0.0009            1.09s\n",
      "        60           0.2394           0.0004            1.06s\n",
      "        70           0.2118           0.0001            1.03s\n",
      "        80           0.2081           0.0005            1.00s\n",
      "        90           0.1960           0.0001            0.98s\n",
      "       100           0.1961           0.0002            0.95s\n",
      "       200           0.1238           0.0000            0.64s\n",
      "       300           0.0998           0.0000            0.32s\n",
      "       400           0.0706           0.0000            0.00s\n",
      "CV score: 0.21276060\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingRegressor梯度提升决策树\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "oof_gbr = np.zeros(len(X_train))\n",
    "predictions_gbr = np.zeros(len(X_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train.iloc[trn_idx]\n",
    "    tr_y = y_train.iloc[trn_idx]\n",
    "    gbr_263 = gbr(n_estimators=400, learning_rate=0.01,subsample=0.65,max_depth=7, min_samples_leaf=20,\n",
    "            max_features=0.22,verbose=1)\n",
    "    gbr_263.fit(tr_x,tr_y)\n",
    "    oof_gbr[val_idx] = gbr_263.predict(X_train.iloc[val_idx])\n",
    "    \n",
    "    predictions_gbr += gbr_263.predict(X_test) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.8f}\".format(mean_squared_error(oof_gbr, target)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c050988",
   "metadata": {},
   "source": [
    "# ExtraTreesRegressor 极端随机森林回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bcd8e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:02:09.340530Z",
     "start_time": "2022-12-06T09:02:04.857637Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 402 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 752 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.217   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=24)]: Using backend ThreadingBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=24)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 152 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 402 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=24)]: Done 752 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=24)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#ExtraTreesRegressor 极端随机森林回归\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "oof_etr = np.zeros(len(X_train))\n",
    "predictions_etr = np.zeros(len(X_test))\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"fold n°{}\".format(fold_+1))\n",
    "    tr_x = X_train.iloc[trn_idx]\n",
    "    tr_y = y_train.iloc[trn_idx]\n",
    "    etr_263 = etr(n_estimators=1000,max_depth=8, min_samples_leaf=12, min_weight_fraction_leaf=0.0,\n",
    "            max_features=0.4,verbose=1,n_jobs=-1)# max_feature：划分时考虑的最大特征数\n",
    "    etr_263.fit(tr_x,tr_y)\n",
    "    oof_etr[val_idx] = etr_263.predict(X_train.iloc[val_idx])\n",
    "    \n",
    "    predictions_etr += etr_263.predict(X_test) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.3f}\".format(mean_squared_error(oof_etr, target)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f14e99",
   "metadata": {},
   "source": [
    "# 集成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "280b32b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T03:13:28.900572Z",
     "start_time": "2022-12-05T03:13:28.880027Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m oof_stack2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(train_stack2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      8\u001b[0m predictions_lr2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(test_stack2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_, (trn_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(folds_stack\u001b[38;5;241m.\u001b[39msplit(train_stack2,\u001b[43mtarget\u001b[49m)):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(fold_))\n\u001b[1;32m     12\u001b[0m     trn_data, trn_y \u001b[38;5;241m=\u001b[39m train_stack2[trn_idx], target\u001b[38;5;241m.\u001b[39miloc[trn_idx]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "# transpose()函数的作用就是调换x,y,z的位置,也就是数组的索引值\n",
    "train_stack2 = np.vstack([oof_lgb_263,oof_gbr_263,oof_rfr_263,oof_etr_263]).transpose()\n",
    "test_stack2 = np.vstack([predictions_lgb_263, predictions_gbr_263,predictions_rfr_263,predictions_etr_263]).transpose()\n",
    "\n",
    "#交叉验证:5折，重复2次\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=7)\n",
    "oof_stack2 = np.zeros(train_stack2.shape[0])\n",
    "predictions_lr2 = np.zeros(test_stack2.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack2, target)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack2[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack2[val_idx], target.iloc[val_idx].values\n",
    "    #Kernel Ridge Regression\n",
    "    lr2 = kr()\n",
    "    lr2.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack2[val_idx] = lr2.predict(val_data)\n",
    "    predictions_lr2 += lr2.predict(test_stack2) / 10\n",
    "    \n",
    "mean_squared_error(target.values, oof_stack2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97474ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_example = pd.read_csv('submit_example.csv',sep=',',encoding='latin-1')\n",
    "submit_example['happiness'] = predictions_lr5\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c521996",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_example.loc[submit_example['happiness']>4.96,'happiness']= 5\n",
    "submit_example.loc[submit_example['happiness']<=1.04,'happiness']= 1\n",
    "submit_example.loc[(submit_example['happiness']>1.96)&(submit_example['happiness']<2.04),'happiness']= 2\n",
    "submit_example.to_csv(\"submision.csv\",index=False)\n",
    "submit_example.happiness.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf03ea2",
   "metadata": {},
   "source": [
    "# 模型参数搜索\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41dc595",
   "metadata": {},
   "outputs": [],
   "source": [
    "####   2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "#把要调整的参数以及其候选值 列出来；\n",
    "param_grid = {\"gamma\":[0.001,0.01,0.1,1,10,100],\n",
    "             \"C\":[0.001,0.01,0.1,1,10,100]}\n",
    "print(\"Parameters:{}\".format(param_grid))\n",
    " \n",
    "grid_search = GridSearchCV(SVC(),param_grid,cv=5) #实例化一个GridSearchCV类,cv交叉验证参数\n",
    "X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=10)\n",
    "grid_search.fit(X_train,y_train) #训练，找到最优的参数，同时使用最优的参数实例化一个新的SVC estimator。\n",
    "print(\"Test set score:{:.2f}\".format(grid_search.score(X_test,y_test)))\n",
    "print(\"Best parameters:{}\".format(grid_search.best_params_))\n",
    "\n",
    "#SVM模型有两个非常重要的参数C与gamma。\n",
    "#C是惩罚系数，即对误差的容忍度（间隔大小,分类准确度）。C越高，说明越不能容忍出现误差,容易过拟合。C越小，容易欠拟合。C过大或过小，泛化能力变差\n",
    "#gamma是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，gamma值越小，支持向量越多。支持向量的个数影响训练与预测的速度。\n",
    "#两者独立"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mytensor]",
   "language": "python",
   "name": "conda-env-mytensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
